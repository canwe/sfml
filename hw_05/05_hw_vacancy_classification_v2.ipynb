{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5: Vacancy Classification SF01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [0] Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import bs4\n",
    "import re\n",
    "from itertools import combinations\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas();\n",
    "\n",
    "import gensim\n",
    "import pymorphy2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RSTATE = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1] Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = pd.read_csv('train.csv', sep='\\t', index_col='id')\n",
    "# test = pd.read_csv('test.csv', sep='\\t', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other = pd.read_csv('other.csv', sep='\\t')\n",
    "# other.index = other.index.set_names('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()    # MorphAnalyzer instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = re.compile(\"([А-ЯЁа-яёA-z]+(-[А-ЯЁа-яёA-z]+)*)\", re.S)    # RegExp for finding russian and english words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(sentence):\n",
    "    '''\n",
    "    Process one sentence (string).\n",
    "    '''\n",
    "\n",
    "    # strip html tags\n",
    "    text = bs4.BeautifulSoup(sentence, 'lxml').get_text()\n",
    "\n",
    "    # tokenize sentence and normalize words + filter out prepositions and conjunctions\n",
    "    words = [(morph.parse(token[0])[0]).normal_form for token in template.findall(text)\n",
    "             if morph.parse(token[0])[0].tag.POS not in ['PREP','CONJ']]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(df):\n",
    "    '''\n",
    "    Strip html tags, tokenize sentences and normalize words.\n",
    "    '''\n",
    "    \n",
    "    text_df = df.copy()\n",
    "    \n",
    "    # process dataframe (columns 'name', 'desciption')\n",
    "    names = []\n",
    "    descs = []\n",
    "    for row in tqdm(text_df.itertuples()):\n",
    "        names.append(process(row.name))\n",
    "        descs.append(process(row.description))\n",
    "        \n",
    "    text_df['name'] = names\n",
    "    text_df['description'] = descs\n",
    "    \n",
    "    del names, descs\n",
    "    \n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess train set and additional data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# other_norm = other.pipe(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_norm = train.pipe(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# test_norm = test.pipe(tokenize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other_norm.to_csv('other_norm.csv', sep='\\t', index_label='id')\n",
    "# train_norm.to_csv('train_norm.csv', sep='\\t', index_label='id')\n",
    "# test_norm.to_csv('test_norm.csv', sep='\\t', index_label='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load from disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = lambda s: s.strip(\"[]\").replace(\"'\", '').split(\", \")\n",
    "other_norm = pd.read_csv('other_norm.csv', sep='\\t', index_col='id', converters={'name': conv, 'description': conv})\n",
    "train_norm = pd.read_csv('train_norm.csv', sep='\\t', index_col='id', converters={'name': conv, 'description': conv})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into train and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_norm, test_size=0.1, random_state=RSTATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] Train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec will be trained on 90% of train data and additional data (file 'other.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = np.concatenate([train_df.name.values+train_df.description.values, \n",
    "                         other_norm.name.values+other_norm.description.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del other_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v_model = gensim.models.Word2Vec(sentences=corpus, size=100, sg=0, window=10, sample=1e-5, workers=4, \n",
    "                                   seed=RSTATE, min_count=1, hs=0, negative=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model.save('w2v_model_cbow_100.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] Identify keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count word frequency by target class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['name'].map(lambda x: ' '.join(x)).values + ' ' + train_df['description'].map(lambda x: ' '.join(x)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_pos = CountVectorizer()\n",
    "counter_neg = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "counts_pos = counter_pos.fit_transform(X[train_df.target==1])\n",
    "counts_neg = counter_neg.fit_transform(X[train_df.target==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, counts_pos, counts_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select most frequent words from positive class as keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keywords(top, k=1):\n",
    "    '''Create keywords from top positive class words.'''\n",
    "    \n",
    "    toppos = list(counter_pos.vocabulary_.keys())[:top]\n",
    "    topneg = list(counter_neg.vocabulary_.keys())[:top*k]    # exclude top*k negative class words\n",
    "    \n",
    "    return [w for w in toppos if w not in topneg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['продавец', 'обязанность', 'консультирование', 'продажа', 'оборудование', 'приём', 'входящая', 'звонок', 'постоянный', 'поиск', 'клиент', 'потенциальный', 'исходящая', 'соблюдение', 'порядок', 'торговый', 'зал']\n"
     ]
    }
   ],
   "source": [
    "print(make_keywords(20, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5] Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_doc(model, tokens, method='mean'):\n",
    "    '''\n",
    "    Create embedding for a single document, represented as a list of tokens.\n",
    "    '''\n",
    "\n",
    "    methods = {\n",
    "        'mean': lambda x: np.mean(x, axis=0),\n",
    "        'sum': lambda x: np.sum(x, axis=0),\n",
    "        'mean_conv': lambda x: np.mean([np.convolve(vx, vy, mode='same') for vx, vy in combinations(x, 2)], axis=0)\n",
    "    }\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            vector = model[token]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        vectors.append(vector)\n",
    "\n",
    "    if vectors == []:\n",
    "        return np.zeros(model.vector_size)    # return all zeros if tokens not in dictionary\n",
    "    else:\n",
    "        return methods[method](vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_to_word(model, sentence_vec, word):\n",
    "    '''\n",
    "    Calculate cosine similarity between sentence embedding and single word embedding.\n",
    "    '''\n",
    "    word_vec = model.wv[word].reshape(1, -1)\n",
    "    return cosine_similarity(sentence_vec, word_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecEmbedding(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Create word2vec embedding for an array of documents.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, model=None, method='mean'):\n",
    "        self.model = model\n",
    "        self.method = method\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df):\n",
    "        '''Process array of documents.'''\n",
    "        \n",
    "        embeddings = np.zeros((df.shape[0], self.model.vector_size))\n",
    "        \n",
    "        for i, row in enumerate(df.itertuples()):\n",
    "            embeddings[i,:] = embed_doc(w2v_model, row.name) + embed_doc(w2v_model, row.description)\n",
    "            \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordsDistanceFeatures(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Add keyword distance features to an array of word2vec-embedded documents.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, keywords, model=None):\n",
    "        self.model = model\n",
    "        self.keywords = keywords\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, emb_docs):\n",
    "        '''Keywords' \"distances\" from emb_docs.'''\n",
    "        \n",
    "        # distance between each document and each of keywords\n",
    "        distances = []\n",
    "        \n",
    "        for kw in self.keywords:\n",
    "            distances.append(np.array([cosine_to_word(w2v_model, s.reshape(1, -1), kw) for s in emb_docs])[:,0])\n",
    "            \n",
    "        return np.hstack([emb_docs, np.hstack(distances)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordsIndicatorFeatures(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Add keyword indicator features to an array of word2vec-embedded documents.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, keywords):\n",
    "        self.keywords = keywords\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, df):\n",
    "        '''Indicator shows whether keyword is part of name/description.'''\n",
    "        \n",
    "        indicators = []\n",
    "        \n",
    "        for row in df.itertuples():\n",
    "            inds = [1 if w in row.description or w in row.name else 0 for w in self.keywords]\n",
    "            indicators.append(np.array(inds))\n",
    "            \n",
    "        return np.vstack(indicators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_pipe(model, kw_dist, kw_ind, method='mean'):\n",
    "    '''Create pipeline for feature generation.'''\n",
    "    \n",
    "    feature_pipe = Pipeline([\n",
    "        ('union', FeatureUnion(\n",
    "            transformer_list=[\n",
    "                ('w2v_feats', Pipeline([\n",
    "                    ('embed', Word2VecEmbedding(model=model, method=method)),\n",
    "                    ('kw_dist', KeywordsDistanceFeatures(keywords=kw_dist, model=model))\n",
    "                ])),\n",
    "                ('kw_ind', KeywordsIndicatorFeatures(keywords=kw_ind))\n",
    "            ]))\n",
    "        ])\n",
    "    \n",
    "    return feature_pipe    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [6] Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_kw = ['продажа', 'товар', 'торговый', 'кассовый', 'клиент', 'кассир', 'клиентский', 'продавец', 'касса', 'продукция']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_feature_pipe(model=w2v_model, kw_dist=custom_kw, kw_ind=custom_kw, method='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = pipe.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=RSTATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_valid = pipe.fit_transform(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9929885839020474"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_score = roc_auc_score(valid_df.target.values, clf.predict_proba(X_valid)[:,1])\n",
    "val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [7] Test prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refit classifier on the whole training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=1000, n_jobs=-1,\n",
       "            oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(np.vstack([X_train, X_valid]), np.concatenate([y_train, valid_df.target.values]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make test prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_norm = pd.read_csv('test_norm.csv', sep='\\t', index_col='id', converters={'name': conv, 'description': conv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_test = pipe.fit_transform(test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame({'target': test_pred}, index = test_norm.index.values)\n",
    "sub.index = sub.index.set_names('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('w2v_cbow_100_ns5_rf_1000_ckw2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
